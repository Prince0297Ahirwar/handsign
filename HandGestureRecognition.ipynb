{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e9ec5bc",
   "metadata": {},
   "source": [
    "# 1. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98d5c871",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 #opencv\n",
    "import numpy as np\n",
    "import os   #helps with path\n",
    "from matplotlib import pyplot as plt #to use plt.imshow()\n",
    "import time              #to measure time between frames \n",
    "import mediapipe as mp   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892c8d64",
   "metadata": {},
   "source": [
    "# 2 . drawingutil and hands module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00f5afd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpDraw=mp.solutions.drawing_utils\n",
    "mpHands=mp.solutions.hands"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf0594b",
   "metadata": {},
   "source": [
    "# 3. Function to detect points on hands and then drawing on the hands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5dc4aa9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB because by default opencv use bgr but we need rgb for mediapipe to process image\n",
    "    image.flags.writeable = False                  # Image is no longer writeable\n",
    "    results = model.process(image)                 # Make prediction\n",
    "    image.flags.writeable = True                   # Image is now writeable \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR\n",
    "    return image, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5466f0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks(image, results):\n",
    "    if results.multi_hand_landmarks:\n",
    "        for num, handsLms in enumerate(results.multi_hand_landmarks):\n",
    "            mpDraw.draw_landmarks(image,handsLms, mpHands.HAND_CONNECTIONS,\n",
    "                             mpDraw.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4), \n",
    "                             mpDraw.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b594ee6d",
   "metadata": {},
   "source": [
    "# 4. function to get handedness ie left or right and to extract point to numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1050f0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(index,results):\n",
    "    label = None\n",
    "    for idx, classification in enumerate(results.multi_handedness):\n",
    "        if classification.classification[0].index == index:\n",
    "            label = classification.classification[0].label\n",
    "#         print(index,idx,label)\n",
    "    if label:\n",
    "        return label\n",
    "    else:\n",
    "        if index == 1:\n",
    "            return get_label(0,results)\n",
    "        elif index == 0:\n",
    "            return get_label(1,results)\n",
    "        else:\n",
    "            return label\n",
    "def extract_keypoints(results):\n",
    "    hands = [np.zeros(21*3),np.zeros(21*3)]\n",
    "    if results.multi_hand_landmarks:\n",
    "        for num, handsLms in enumerate(results.multi_hand_landmarks):\n",
    "            label = get_label(num,results)\n",
    "            if label == 'Right':\n",
    "                hands[0] = np.array([[res.x, res.y, res.z] for res in handsLms.landmark]).flatten()\n",
    "            if label == 'Left':\n",
    "                hands[1] = np.array([[res.x, res.y, res.z] for res in handsLms.landmark]).flatten()\n",
    "            \n",
    "    return np.concatenate(hands)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b46b161",
   "metadata": {},
   "source": [
    "# 5. Number of videos of dataset and videolength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aeefb2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thirty videos worth of data\n",
    "no_videos = 200\n",
    "\n",
    "# Videos are going to be 30 frames in length\n",
    "video_length = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16aec61e",
   "metadata": {},
   "source": [
    "# 6. defining path of dataset and also path where we will store the processed data ie folder p1data in current directory\n",
    "dataset link <a href=\"https://drive.google.com/drive/folders/1RZaXXy3pr7YLSv1jKFXDgmNC2nzkgV1P?usp=sharing\">DATASET<a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "012205b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91963\\Desktop\\HandSignRecognition\\DEMO\n"
     ]
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "print(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "273eb3f1",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A',\n",
       " 'B',\n",
       " 'C',\n",
       " 'D',\n",
       " 'del',\n",
       " 'E',\n",
       " 'F',\n",
       " 'G',\n",
       " 'H',\n",
       " 'I',\n",
       " 'J',\n",
       " 'K',\n",
       " 'L',\n",
       " 'M',\n",
       " 'N',\n",
       " 'nothing',\n",
       " 'O',\n",
       " 'P',\n",
       " 'Q',\n",
       " 'R',\n",
       " 'S',\n",
       " 'space',\n",
       " 'T',\n",
       " 'U',\n",
       " 'V',\n",
       " 'W',\n",
       " 'X',\n",
       " 'Y',\n",
       " 'Z']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datapath = os.path.join('KgData/asl4g/train/')\n",
    "os.listdir(datapath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f6b6028",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path for numpy array of data\n",
    "DATA_PATH = os.path.join('p1Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f619bab6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints', 'HandGestureRecognition.ipynb', 'KgData']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(cwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a490480",
   "metadata": {},
   "source": [
    "# 7. defining the categories of signs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "835aa073",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = np.array(['A','B','C','D','E','F','G','H','I','J','del','space','nothing'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bb242e",
   "metadata": {},
   "source": [
    "# 8. making directory to store the processed data in p1Data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2f584af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in categories: \n",
    "    for i in range(no_videos):\n",
    "        try: \n",
    "            os.makedirs(os.path.join(DATA_PATH,category, str(i)))\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "746e1e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.walk()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8d3d84",
   "metadata": {},
   "source": [
    "# 9. checking data is properly fetching  and also the camera feed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a1c8f929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A  of Length 8458\n",
      "B  of Length 8309\n",
      "C  of Length 8146\n",
      "D  of Length 7629\n",
      "E  of Length 7744\n",
      "F  of Length 8031\n",
      "G  of Length 7844\n",
      "H  of Length 7906\n",
      "I  of Length 7953\n",
      "J  of Length 7503\n",
      "del  of Length 6836\n",
      "space  of Length 7071\n",
      "nothing  of Length 7030\n"
     ]
    }
   ],
   "source": [
    "for category in categories:\n",
    "    path = os.path.join(datapath,category)\n",
    "    print(category,\" of Length\",len(os.listdir(path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "d6155774",
   "metadata": {},
   "outputs": [],
   "source": [
    "catArray1 = []\n",
    "catArray2 = []\n",
    "t = True\n",
    "for category in categories:\n",
    "    path = os.path.join(datapath,category)\n",
    "    img = cv2.imread(os.path.join(path,os.listdir(path)[1]))\n",
    "    img = cv2.resize(img,(180,200))\n",
    "    cv2.putText(img,category,(10,50),cv2.FONT_HERSHEY_SIMPLEX,1, (0,0,255),2, cv2.LINE_AA)\n",
    "    if t:\n",
    "        catArray1.append(img)\n",
    "        t = False\n",
    "    else:\n",
    "        t = True\n",
    "        catArray2.append(img)\n",
    "    cv2.imshow(\"frame\",img)\n",
    "    if cv2.waitKey(1000) & 0xFF == ord('q'):\n",
    "        break\n",
    "cv2.destroyAllWindows()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "708d56b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cata = cv2.hconcat(catArray1)\n",
    "catb = cv2.hconcat(catArray2)\n",
    "cv2.imshow('categories1',cata)\n",
    "cv2.imshow('categories2',catb)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "aaa7cfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in categories:\n",
    "    path = os.path.join(datapath,category)\n",
    "    for imgpath in os.listdir(path):\n",
    "        img_array = cv2.imread(os.path.join(path,imgpath))\n",
    "        img_array = cv2.resize(img_array,(480,640))\n",
    "        image, results = mediapipe_detection(img_array, mpHands.Hands())\n",
    "        draw_landmarks(image, results)\n",
    "        cv2.imshow(\"frame without detection\",img_array)\n",
    "        cv2.imshow(\"frame with detection\",image)\n",
    "        if cv2.waitKey(1000) & 0xFF == ord('q'):\n",
    "                    break\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "58e7d8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0) #creating video capture object\n",
    "## Set mediapipe model\n",
    "\n",
    "while cap.isOpened():\n",
    "    #reading feed current frame\n",
    "    ret,frame = cap.read() \n",
    "    flipframe = cv2.flip(frame,1)\n",
    "    cv2.imshow(\"Open Cv1\",flipframe) \n",
    "    cv2.imshow('o2',frame)\n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'): #wait for 34 milli second to check if q is pressed on keyboard\n",
    "        break \n",
    "cap.release() #release our webcame\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351308cb",
   "metadata": {},
   "source": [
    "# 10. Extracting keypoints using extract_keypoint funtion and for every frame   #  for every category we are creating 200 videos and every video contains 30 frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7d4dbf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mpHands.Hands() as model:\n",
    "    for category in categories:f\n",
    "        path = os.path.join(datapath,category)\n",
    "        adl = os.listdir(path)\n",
    "        cdl = 0\n",
    "        for i in range(no_videos):\n",
    "            b = False\n",
    "            for j in range(video_length):\n",
    "                image = cv2.imread(os.path.join(path,adl[cdl]))\n",
    "#                 image = cv2.flip(image,1)\n",
    "                image = cv2.resize(image,(640,480))\n",
    "                image, results = mediapipe_detection(image, model)\n",
    "                draw_landmarks(image, results)\n",
    "#                 print(results.multi_handedness)\n",
    "                cdl += 1\n",
    "                cv2.putText(image,'f no {} Vno {} c {}'.format(j,i,category), (10,20), \n",
    "                                   cv2.FONT_HERSHEY_SIMPLEX,0.5, (150, 0, 255), 1, cv2.LINE_AA)\n",
    "                cv2.imshow(\"frame\",image)\n",
    "                # NEW Export keypoints\n",
    "                keypoints = extract_keypoints(results)\n",
    "                npy_path = os.path.join(DATA_PATH, category, str(i), str(j))\n",
    "                np.save(npy_path, keypoints)\n",
    "                \n",
    "                if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                    b = True\n",
    "                    break\n",
    "            if b:\n",
    "                break\n",
    "\n",
    "\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bc0eaa",
   "metadata": {},
   "source": [
    "# 11. converting categories using one hot encoding and splitting data into train and test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "82e6a16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "label_map = {label:num for num, label in enumerate(categories)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ab491d2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A': 0,\n",
       " 'B': 1,\n",
       " 'C': 2,\n",
       " 'D': 3,\n",
       " 'E': 4,\n",
       " 'F': 5,\n",
       " 'G': 6,\n",
       " 'H': 7,\n",
       " 'I': 8,\n",
       " 'J': 9,\n",
       " 'del': 10,\n",
       " 'space': 11,\n",
       " 'nothing': 12}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "62dc19b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "videos, labels = [], []\n",
    "for category in categories:\n",
    "    for video in range(no_videos):\n",
    "        window = []\n",
    "        for frame_num in range(video_length):\n",
    "            res = np.load(os.path.join(DATA_PATH, category, str(video), \"{}.npy\".format(frame_num)))\n",
    "            window.append(res)\n",
    "        videos.append(window)\n",
    "        labels.append(label_map[category])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "228c1a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(videos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "61000f7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2600, 30, 126)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a7179e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(labels).astype(int) #one hot encoding to convert categorial variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7a52a5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ac3466e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(520, 13)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "df7209f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2080, 30, 126)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce67774b",
   "metadata": {},
   "source": [
    "# 12. Building model and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2ec3df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04617421",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "ACCURACY_THRESHOLD = 0.95\n",
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        # print(logs.get('acc'))\n",
    "        # print(logs.get('categorical_accuracy'))\n",
    "        if(logs.get('categorical_accuracy') > ACCURACY_THRESHOLD):\n",
    "            # print(logs.get('acc'))\n",
    "            # print(logs.get('categorical_accuracy'))\n",
    "            print(\"\\nReached %2.2f%% accuracy, so stopping training!!\" %(ACCURACY_THRESHOLD*100))\n",
    "            self.model.stop_training = True\n",
    "\n",
    "# Instantiate a callback object\n",
    "callbacks = myCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0a95d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = os.path.join('trainLogsdLstm')\n",
    "tb_callback = TensorBoard(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4340bf55",
   "metadata": {},
   "outputs": [],
   "source": [
    "mycallbacks = [callbacks,tb_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21d26afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(64, return_sequences=True, activation='relu', input_shape=(30,126)))\n",
    "model.add(LSTM(128, return_sequences=True, activation='relu'))\n",
    "model.add(LSTM(64, return_sequences=False, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(categories.shape[0], activation='softmax'))\n",
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cb2bee29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      " 1/65 [..............................] - ETA: 0s - loss: 2.5634 - categorical_accuracy: 0.0625WARNING:tensorflow:From C:\\Users\\91963\\anaconda3\\envs\\py37gpu\\lib\\site-packages\\tensorflow\\python\\ops\\summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      " 2/65 [..............................] - ETA: 13s - loss: 2.5626 - categorical_accuracy: 0.0781WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1490s vs `on_train_batch_end` time: 0.2845s). Check your callbacks.\n",
      "65/65 [==============================] - 8s 123ms/step - loss: 2.4880 - categorical_accuracy: 0.1500\n",
      "Epoch 2/500\n",
      "65/65 [==============================] - 7s 104ms/step - loss: 2.0757 - categorical_accuracy: 0.2644\n",
      "Epoch 3/500\n",
      "65/65 [==============================] - 7s 104ms/step - loss: 1.8568 - categorical_accuracy: 0.3433\n",
      "Epoch 4/500\n",
      "65/65 [==============================] - 7s 105ms/step - loss: 1.4354 - categorical_accuracy: 0.4885\n",
      "Epoch 5/500\n",
      "65/65 [==============================] - 7s 107ms/step - loss: 1.2954 - categorical_accuracy: 0.5231\n",
      "Epoch 6/500\n",
      "65/65 [==============================] - 7s 113ms/step - loss: 1.0364 - categorical_accuracy: 0.6260\n",
      "Epoch 7/500\n",
      "65/65 [==============================] - 7s 110ms/step - loss: 0.9320 - categorical_accuracy: 0.6793\n",
      "Epoch 8/500\n",
      "65/65 [==============================] - 7s 103ms/step - loss: 0.8585 - categorical_accuracy: 0.7082\n",
      "Epoch 9/500\n",
      "65/65 [==============================] - 7s 109ms/step - loss: 0.7519 - categorical_accuracy: 0.7572\n",
      "Epoch 10/500\n",
      "65/65 [==============================] - 7s 105ms/step - loss: 0.6978 - categorical_accuracy: 0.7712\n",
      "Epoch 11/500\n",
      "65/65 [==============================] - 8s 131ms/step - loss: 0.5899 - categorical_accuracy: 0.8240\n",
      "Epoch 12/500\n",
      "65/65 [==============================] - 8s 127ms/step - loss: 0.7758 - categorical_accuracy: 0.7688\n",
      "Epoch 13/500\n",
      "65/65 [==============================] - 9s 141ms/step - loss: 0.7404 - categorical_accuracy: 0.7913\n",
      "Epoch 14/500\n",
      "65/65 [==============================] - 9s 138ms/step - loss: 0.6092 - categorical_accuracy: 0.8236\n",
      "Epoch 15/500\n",
      "65/65 [==============================] - 9s 135ms/step - loss: 0.4654 - categorical_accuracy: 0.8668\n",
      "Epoch 16/500\n",
      "65/65 [==============================] - 9s 134ms/step - loss: 0.4486 - categorical_accuracy: 0.8726\n",
      "Epoch 17/500\n",
      "65/65 [==============================] - 8s 130ms/step - loss: 0.4065 - categorical_accuracy: 0.8865\n",
      "Epoch 18/500\n",
      "65/65 [==============================] - 9s 138ms/step - loss: 0.4169 - categorical_accuracy: 0.8798\n",
      "Epoch 19/500\n",
      "65/65 [==============================] - 9s 131ms/step - loss: 0.4261 - categorical_accuracy: 0.8817\n",
      "Epoch 20/500\n",
      "65/65 [==============================] - 9s 136ms/step - loss: 0.4351 - categorical_accuracy: 0.8683\n",
      "Epoch 21/500\n",
      "65/65 [==============================] - 9s 132ms/step - loss: 0.4026 - categorical_accuracy: 0.8904\n",
      "Epoch 22/500\n",
      "65/65 [==============================] - 9s 132ms/step - loss: 0.6286 - categorical_accuracy: 0.8034\n",
      "Epoch 23/500\n",
      "65/65 [==============================] - 8s 130ms/step - loss: 0.3467 - categorical_accuracy: 0.9106\n",
      "Epoch 24/500\n",
      "65/65 [==============================] - 9s 131ms/step - loss: 0.4359 - categorical_accuracy: 0.8712\n",
      "Epoch 25/500\n",
      "65/65 [==============================] - 9s 135ms/step - loss: 0.3160 - categorical_accuracy: 0.9091\n",
      "Epoch 26/500\n",
      "65/65 [==============================] - 8s 131ms/step - loss: 0.3465 - categorical_accuracy: 0.8981\n",
      "Epoch 27/500\n",
      "65/65 [==============================] - 8s 130ms/step - loss: 0.4125 - categorical_accuracy: 0.8894\n",
      "Epoch 28/500\n",
      "65/65 [==============================] - 9s 131ms/step - loss: 0.2749 - categorical_accuracy: 0.9255\n",
      "Epoch 29/500\n",
      "65/65 [==============================] - 8s 127ms/step - loss: 1.6837 - categorical_accuracy: 0.7111\n",
      "Epoch 30/500\n",
      "65/65 [==============================] - 9s 136ms/step - loss: 1.0853 - categorical_accuracy: 0.6341\n",
      "Epoch 31/500\n",
      "65/65 [==============================] - 9s 131ms/step - loss: 0.6195 - categorical_accuracy: 0.8139\n",
      "Epoch 32/500\n",
      "65/65 [==============================] - 9s 138ms/step - loss: 0.5638 - categorical_accuracy: 0.8332\n",
      "Epoch 33/500\n",
      "65/65 [==============================] - 9s 131ms/step - loss: 0.4489 - categorical_accuracy: 0.8736\n",
      "Epoch 34/500\n",
      "65/65 [==============================] - 9s 132ms/step - loss: 0.4734 - categorical_accuracy: 0.8639\n",
      "Epoch 35/500\n",
      "65/65 [==============================] - 9s 133ms/step - loss: 0.4294 - categorical_accuracy: 0.8654\n",
      "Epoch 36/500\n",
      "65/65 [==============================] - 9s 132ms/step - loss: 0.4073 - categorical_accuracy: 0.8846\n",
      "Epoch 37/500\n",
      "65/65 [==============================] - 9s 132ms/step - loss: 0.4421 - categorical_accuracy: 0.8721\n",
      "Epoch 38/500\n",
      "65/65 [==============================] - 8s 130ms/step - loss: 1.9571 - categorical_accuracy: 0.5548\n",
      "Epoch 39/500\n",
      "65/65 [==============================] - 8s 124ms/step - loss: 0.8078 - categorical_accuracy: 0.7202\n",
      "Epoch 40/500\n",
      "65/65 [==============================] - 8s 126ms/step - loss: 0.5869 - categorical_accuracy: 0.8082\n",
      "Epoch 41/500\n",
      "65/65 [==============================] - 8s 121ms/step - loss: 0.5044 - categorical_accuracy: 0.8462\n",
      "Epoch 42/500\n",
      "65/65 [==============================] - 7s 107ms/step - loss: 0.4739 - categorical_accuracy: 0.8615\n",
      "Epoch 43/500\n",
      "65/65 [==============================] - 7s 109ms/step - loss: 0.4052 - categorical_accuracy: 0.8827\n",
      "Epoch 44/500\n",
      "65/65 [==============================] - 7s 105ms/step - loss: 0.4369 - categorical_accuracy: 0.8731\n",
      "Epoch 45/500\n",
      "65/65 [==============================] - 7s 112ms/step - loss: 0.3751 - categorical_accuracy: 0.8933\n",
      "Epoch 46/500\n",
      "65/65 [==============================] - 7s 108ms/step - loss: 0.3785 - categorical_accuracy: 0.9024\n",
      "Epoch 47/500\n",
      "65/65 [==============================] - 7s 107ms/step - loss: 2.2514 - categorical_accuracy: 0.5870\n",
      "Epoch 48/500\n",
      "65/65 [==============================] - 7s 110ms/step - loss: 1.6503 - categorical_accuracy: 0.3942\n",
      "Epoch 49/500\n",
      "65/65 [==============================] - 8s 119ms/step - loss: 1.1020 - categorical_accuracy: 0.6087\n",
      "Epoch 50/500\n",
      "65/65 [==============================] - 7s 105ms/step - loss: 0.9135 - categorical_accuracy: 0.6788\n",
      "Epoch 51/500\n",
      "65/65 [==============================] - 7s 110ms/step - loss: 0.7399 - categorical_accuracy: 0.7389\n",
      "Epoch 52/500\n",
      "65/65 [==============================] - 7s 107ms/step - loss: 0.7102 - categorical_accuracy: 0.7442\n",
      "Epoch 53/500\n",
      "65/65 [==============================] - 7s 115ms/step - loss: 0.5721 - categorical_accuracy: 0.7966\n",
      "Epoch 54/500\n",
      "65/65 [==============================] - 7s 110ms/step - loss: 0.6153 - categorical_accuracy: 0.7865\n",
      "Epoch 55/500\n",
      "65/65 [==============================] - 7s 110ms/step - loss: 0.5241 - categorical_accuracy: 0.8231\n",
      "Epoch 56/500\n",
      "65/65 [==============================] - 7s 108ms/step - loss: 0.7233 - categorical_accuracy: 0.7577\n",
      "Epoch 57/500\n",
      "65/65 [==============================] - 8s 117ms/step - loss: 0.4603 - categorical_accuracy: 0.8562\n",
      "Epoch 58/500\n",
      "65/65 [==============================] - 7s 103ms/step - loss: 0.5232 - categorical_accuracy: 0.8452\n",
      "Epoch 59/500\n",
      "65/65 [==============================] - 7s 107ms/step - loss: 0.5471 - categorical_accuracy: 0.8255\n",
      "Epoch 60/500\n",
      "65/65 [==============================] - 7s 103ms/step - loss: 0.4462 - categorical_accuracy: 0.8687\n",
      "Epoch 61/500\n",
      "65/65 [==============================] - 7s 103ms/step - loss: 0.5680 - categorical_accuracy: 0.8207\n",
      "Epoch 62/500\n",
      "65/65 [==============================] - 7s 103ms/step - loss: 0.4190 - categorical_accuracy: 0.8731\n",
      "Epoch 63/500\n",
      "65/65 [==============================] - 7s 105ms/step - loss: 0.4980 - categorical_accuracy: 0.8462\n",
      "Epoch 64/500\n",
      "65/65 [==============================] - 7s 107ms/step - loss: 0.3964 - categorical_accuracy: 0.8971\n",
      "Epoch 65/500\n",
      "65/65 [==============================] - 7s 107ms/step - loss: 0.3757 - categorical_accuracy: 0.8913\n",
      "Epoch 66/500\n",
      "65/65 [==============================] - 7s 104ms/step - loss: 0.4262 - categorical_accuracy: 0.8726\n",
      "Epoch 67/500\n",
      "65/65 [==============================] - 7s 106ms/step - loss: 0.4440 - categorical_accuracy: 0.8644\n",
      "Epoch 68/500\n",
      "65/65 [==============================] - 8s 118ms/step - loss: 0.3473 - categorical_accuracy: 0.9014\n",
      "Epoch 69/500\n",
      "65/65 [==============================] - 7s 113ms/step - loss: 0.3685 - categorical_accuracy: 0.8904\n",
      "Epoch 70/500\n",
      "65/65 [==============================] - 8s 123ms/step - loss: 0.3139 - categorical_accuracy: 0.9163\n",
      "Epoch 71/500\n",
      "65/65 [==============================] - 8s 125ms/step - loss: 0.3988 - categorical_accuracy: 0.8865\n",
      "Epoch 72/500\n",
      "65/65 [==============================] - 7s 106ms/step - loss: 0.5840 - categorical_accuracy: 0.8404\n",
      "Epoch 73/500\n",
      "65/65 [==============================] - 8s 117ms/step - loss: 0.3676 - categorical_accuracy: 0.8952\n",
      "Epoch 74/500\n",
      "65/65 [==============================] - 7s 109ms/step - loss: 0.3688 - categorical_accuracy: 0.8889\n",
      "Epoch 75/500\n",
      "65/65 [==============================] - 6s 100ms/step - loss: 0.3377 - categorical_accuracy: 0.8995\n",
      "Epoch 76/500\n",
      "65/65 [==============================] - 7s 115ms/step - loss: 0.3986 - categorical_accuracy: 0.8899\n",
      "Epoch 77/500\n",
      "65/65 [==============================] - 8s 116ms/step - loss: 0.3193 - categorical_accuracy: 0.9111\n",
      "Epoch 78/500\n",
      "65/65 [==============================] - 7s 106ms/step - loss: 0.2878 - categorical_accuracy: 0.9139\n",
      "Epoch 79/500\n",
      "65/65 [==============================] - 8s 125ms/step - loss: 0.3602 - categorical_accuracy: 0.9096\n",
      "Epoch 80/500\n",
      "65/65 [==============================] - 9s 137ms/step - loss: 0.4905 - categorical_accuracy: 0.8572\n",
      "Epoch 81/500\n",
      "65/65 [==============================] - 9s 132ms/step - loss: 0.3816 - categorical_accuracy: 0.8933\n",
      "Epoch 82/500\n",
      "65/65 [==============================] - 9s 134ms/step - loss: 0.3097 - categorical_accuracy: 0.9149\n",
      "Epoch 83/500\n",
      "65/65 [==============================] - 9s 131ms/step - loss: 0.3027 - categorical_accuracy: 0.9135\n",
      "Epoch 84/500\n",
      "65/65 [==============================] - 9s 132ms/step - loss: 0.3402 - categorical_accuracy: 0.9005\n",
      "Epoch 85/500\n",
      "65/65 [==============================] - 9s 137ms/step - loss: 0.2528 - categorical_accuracy: 0.9327\n",
      "Epoch 86/500\n",
      "65/65 [==============================] - 8s 129ms/step - loss: 0.2651 - categorical_accuracy: 0.9274\n",
      "Epoch 87/500\n",
      "65/65 [==============================] - 9s 134ms/step - loss: 0.3176 - categorical_accuracy: 0.9120\n",
      "Epoch 88/500\n",
      "65/65 [==============================] - 9s 133ms/step - loss: 0.2454 - categorical_accuracy: 0.9341\n",
      "Epoch 89/500\n",
      "65/65 [==============================] - 8s 128ms/step - loss: 0.3212 - categorical_accuracy: 0.9101\n",
      "Epoch 90/500\n",
      "65/65 [==============================] - 8s 129ms/step - loss: 0.2876 - categorical_accuracy: 0.9192\n",
      "Epoch 91/500\n",
      "65/65 [==============================] - 8s 130ms/step - loss: 0.2279 - categorical_accuracy: 0.9389\n",
      "Epoch 92/500\n",
      "65/65 [==============================] - 8s 128ms/step - loss: 0.3319 - categorical_accuracy: 0.9048\n",
      "Epoch 93/500\n",
      "65/65 [==============================] - 9s 135ms/step - loss: 0.2319 - categorical_accuracy: 0.9389\n",
      "Epoch 94/500\n",
      "65/65 [==============================] - 9s 134ms/step - loss: 0.2182 - categorical_accuracy: 0.9433\n",
      "Epoch 95/500\n",
      "65/65 [==============================] - 9s 134ms/step - loss: 0.2630 - categorical_accuracy: 0.9279\n",
      "Epoch 96/500\n",
      "65/65 [==============================] - 8s 130ms/step - loss: 0.2626 - categorical_accuracy: 0.9245\n",
      "Epoch 97/500\n",
      "65/65 [==============================] - 8s 129ms/step - loss: 0.2390 - categorical_accuracy: 0.9356\n",
      "Epoch 98/500\n",
      "65/65 [==============================] - 9s 134ms/step - loss: 0.2440 - categorical_accuracy: 0.9332\n",
      "Epoch 99/500\n",
      "65/65 [==============================] - 8s 129ms/step - loss: 0.2551 - categorical_accuracy: 0.9298\n",
      "Epoch 100/500\n",
      "65/65 [==============================] - 8s 130ms/step - loss: 0.2559 - categorical_accuracy: 0.9284\n",
      "Epoch 101/500\n",
      "65/65 [==============================] - 8s 131ms/step - loss: 0.3617 - categorical_accuracy: 0.9096\n",
      "Epoch 102/500\n",
      "65/65 [==============================] - 8s 127ms/step - loss: 0.4413 - categorical_accuracy: 0.8731\n",
      "Epoch 103/500\n",
      "65/65 [==============================] - 8s 127ms/step - loss: 0.2276 - categorical_accuracy: 0.9442\n",
      "Epoch 104/500\n",
      "65/65 [==============================] - 9s 137ms/step - loss: 0.2249 - categorical_accuracy: 0.9413\n",
      "Epoch 105/500\n",
      "65/65 [==============================] - 9s 133ms/step - loss: 0.2104 - categorical_accuracy: 0.9452\n",
      "Epoch 106/500\n",
      "65/65 [==============================] - 7s 114ms/step - loss: 0.2614 - categorical_accuracy: 0.9322\n",
      "Epoch 107/500\n",
      "65/65 [==============================] - 7s 105ms/step - loss: 0.2970 - categorical_accuracy: 0.9183\n",
      "Epoch 108/500\n",
      "65/65 [==============================] - 7s 103ms/step - loss: 0.2214 - categorical_accuracy: 0.9399\n",
      "Epoch 109/500\n",
      "65/65 [==============================] - 7s 106ms/step - loss: 0.2278 - categorical_accuracy: 0.9375\n",
      "Epoch 110/500\n",
      "65/65 [==============================] - 7s 106ms/step - loss: 0.2308 - categorical_accuracy: 0.9375\n",
      "Epoch 111/500\n",
      "65/65 [==============================] - 7s 104ms/step - loss: 0.2674 - categorical_accuracy: 0.9288\n",
      "Epoch 112/500\n",
      "65/65 [==============================] - 7s 108ms/step - loss: 0.2315 - categorical_accuracy: 0.9389\n",
      "Epoch 113/500\n",
      "65/65 [==============================] - 7s 106ms/step - loss: 0.6548 - categorical_accuracy: 0.8721\n",
      "Epoch 114/500\n",
      "65/65 [==============================] - 7s 107ms/step - loss: 0.4116 - categorical_accuracy: 0.8784\n",
      "Epoch 115/500\n",
      "65/65 [==============================] - 7s 110ms/step - loss: 0.3040 - categorical_accuracy: 0.9125\n",
      "Epoch 116/500\n",
      "65/65 [==============================] - 7s 113ms/step - loss: 0.3264 - categorical_accuracy: 0.9048\n",
      "Epoch 117/500\n",
      "65/65 [==============================] - 8s 116ms/step - loss: 0.3572 - categorical_accuracy: 0.8933\n",
      "Epoch 118/500\n",
      "65/65 [==============================] - 7s 115ms/step - loss: 0.2501 - categorical_accuracy: 0.9288\n",
      "Epoch 119/500\n",
      "65/65 [==============================] - 8s 121ms/step - loss: 0.2364 - categorical_accuracy: 0.9365\n",
      "Epoch 120/500\n",
      "65/65 [==============================] - 7s 101ms/step - loss: 0.3206 - categorical_accuracy: 0.9072\n",
      "Epoch 121/500\n",
      "65/65 [==============================] - 7s 103ms/step - loss: 0.2295 - categorical_accuracy: 0.9375\n",
      "Epoch 122/500\n",
      "65/65 [==============================] - 7s 107ms/step - loss: 0.2124 - categorical_accuracy: 0.9438\n",
      "Epoch 123/500\n",
      "65/65 [==============================] - 7s 111ms/step - loss: 0.2324 - categorical_accuracy: 0.9351\n",
      "Epoch 124/500\n",
      "65/65 [==============================] - 7s 101ms/step - loss: 0.2456 - categorical_accuracy: 0.9332\n",
      "Epoch 125/500\n",
      "65/65 [==============================] - 7s 103ms/step - loss: 0.2145 - categorical_accuracy: 0.9452\n",
      "Epoch 126/500\n",
      "65/65 [==============================] - 7s 103ms/step - loss: 0.2170 - categorical_accuracy: 0.9442\n",
      "Epoch 127/500\n",
      "65/65 [==============================] - 6s 99ms/step - loss: 0.2604 - categorical_accuracy: 0.9303\n",
      "Epoch 128/500\n",
      "65/65 [==============================] - 7s 102ms/step - loss: 0.2153 - categorical_accuracy: 0.9404\n",
      "Epoch 129/500\n",
      "65/65 [==============================] - 7s 106ms/step - loss: 0.2108 - categorical_accuracy: 0.9438\n",
      "Epoch 130/500\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.1902 - categorical_accuracy: 0.9519\n",
      "Reached 95.00% accuracy, so stopping training!!\n",
      "65/65 [==============================] - 6s 89ms/step - loss: 0.1902 - categorical_accuracy: 0.9519\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1f735c0a0c8>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=500, callbacks=mycallbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8705383d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 30, 64)            48896     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 30, 128)           98816     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 64)                49408     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 13)                429       \n",
      "=================================================================\n",
      "Total params: 220,365\n",
      "Trainable params: 220,365\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cf4f65",
   "metadata": {},
   "source": [
    "# 13. Make prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "58ea69b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c5d8593d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'F'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories[np.argmax(res[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b2913b9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'F'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories[np.argmax(y_test[0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bd3368",
   "metadata": {},
   "source": [
    "# 14. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fc17d506",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model2.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f81cf27",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a5fe456",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('model1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af78a3ad",
   "metadata": {},
   "source": [
    "# 15. Evaluation using Confusion Matrix and Accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ea0957b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "53246345",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5f7f3046",
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrue = np.argmax(y_test, axis=1).tolist()\n",
    "yhat = np.argmax(yhat, axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9a0f6c09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[473,   6],\n",
       "        [  4,  37]],\n",
       "\n",
       "       [[476,   2],\n",
       "        [  4,  38]],\n",
       "\n",
       "       [[475,   2],\n",
       "        [  5,  38]],\n",
       "\n",
       "       [[475,   3],\n",
       "        [  0,  42]],\n",
       "\n",
       "       [[476,   2],\n",
       "        [  4,  38]],\n",
       "\n",
       "       [[485,   2],\n",
       "        [  1,  32]],\n",
       "\n",
       "       [[483,   3],\n",
       "        [  0,  34]],\n",
       "\n",
       "       [[472,   1],\n",
       "        [  1,  46]],\n",
       "\n",
       "       [[474,   1],\n",
       "        [  7,  38]],\n",
       "\n",
       "       [[483,   0],\n",
       "        [  5,  32]],\n",
       "\n",
       "       [[472,   1],\n",
       "        [  8,  39]],\n",
       "\n",
       "       [[487,   2],\n",
       "        [  3,  28]],\n",
       "\n",
       "       [[467,  17],\n",
       "        [  0,  36]]], dtype=int64)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multilabel_confusion_matrix(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b3f5b092",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9192307692307692"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf71fac",
   "metadata": {},
   "source": [
    "# 16. Testing in real Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c1917145",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [(245,117,16), (117,245,16), (16,117,245),(166,107,245),(106,167,245),(106,117,205),(161,107,25),(130,187,245),(106,117,205),(106,100,245),(196,107,245), (117,245,16), (16,117,245)]\n",
    "def prob_viz(res, actions, input_frame, colors):\n",
    "    output_frame = input_frame.copy()\n",
    "    for num, prob in enumerate(res):\n",
    "        cv2.rectangle(output_frame, (0,60+num*40), (int(prob*100), 90+num*40), colors[num], -1)\n",
    "        cv2.putText(output_frame, actions[num], (0, 85+num*40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "        \n",
    "    return output_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "55cb2d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "back = cv2.imread('board.jpg')\n",
    "back = cv2.resize(back,(800,800))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85285225",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mpHands' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-b6e42ad4f913>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mcap\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVideoCapture\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# Set mediapipe model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mmpHands\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mHands\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mhand\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[0mcap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misOpened\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mboard\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'mpHands' is not defined"
     ]
    }
   ],
   "source": [
    "# 1. New detection variables\n",
    "sequence = []\n",
    "sentence = []\n",
    "threshold = 0.8\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "# Set mediapipe model \n",
    "with mpHands.Hands() as hand:\n",
    "    while cap.isOpened():\n",
    "        board = back\n",
    "        # Read feed\n",
    "        ret, frame = cap.read()\n",
    "        # Make detections\n",
    "        image, results = mediapipe_detection(frame,hand)\n",
    "\n",
    "        \n",
    "        # Draw landmarks\n",
    "        draw_landmarks(image, results)\n",
    "        # 2. Prediction logic\n",
    "        keypoints = extract_keypoints(results)\n",
    "\n",
    "        sequence.append(keypoints)\n",
    "#         print(sequence)\n",
    "        sequence = sequence[-30:]\n",
    "#         print(sequence)\n",
    "        \n",
    "        if len(sequence) == 30:\n",
    "            res = model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "        #3. Viz logic\n",
    "            if res[np.argmax(res)] > threshold: \n",
    "                if len(sentence) > 0: \n",
    "                    if categories[np.argmax(res)] != sentence[-1]:\n",
    "                        sentence.append(categories[np.argmax(res)])\n",
    "                else:\n",
    "                    sentence.append(categories[np.argmax(res)])\n",
    "\n",
    "            if len(sentence) > 30: \n",
    "                sentence = sentence[-25:]\n",
    "\n",
    "            # Viz probabilities\n",
    "            board = prob_viz(res, categories,board, colors)\n",
    "            \n",
    "        cv2.rectangle(image, (0,0), (640, 40), (245, 117, 16), -1)\n",
    "        cv2.putText(image, ''.join(sentence).replace('-',' '), (3,30), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        \n",
    "        # Show to screen\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "        cv2.imshow('viz',board)\n",
    "\n",
    "        # Break gracefully\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726ef1f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6360aaf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9ef6d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e8fe43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b549aad5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc049fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ebbbbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c840ed9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96724c08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e71d0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2deb95bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa4eab6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
